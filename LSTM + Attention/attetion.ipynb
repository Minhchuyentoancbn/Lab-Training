{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad446f15-58c4-4f44-8dfc-231dd396bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import tensorflow.keras.initializers as initializers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "import tensorflow.keras.constraints as constraints\n",
    "\n",
    "from preprocess import gen_data, encode_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b179199-193b-4311-a7dc-eec22e7b0195",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCES = 50\n",
    "MAX_WORDS_PER_SENTENCE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f0f097-6559-482c-83f9-1a543b598386",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6fc9981-2e71-4506-a254-693eb0802ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 0-alt.atheism\n",
      "Processing: 1-comp.graphics\n",
      "Processing: 2-comp.os.ms-windows.misc\n",
      "Processing: 3-comp.sys.ibm.pc.hardware\n",
      "Processing: 4-comp.sys.mac.hardware\n",
      "Processing: 5-comp.windows.x\n",
      "Processing: 6-misc.forsale\n",
      "Processing: 7-rec.autos\n",
      "Processing: 8-rec.motorcycles\n",
      "Processing: 9-rec.sport.baseball\n",
      "Processing: 10-rec.sport.hockey\n",
      "Processing: 11-sci.crypt\n",
      "Processing: 12-sci.electronics\n",
      "Processing: 13-sci.med\n",
      "Processing: 14-sci.space\n",
      "Processing: 15-soc.religion.christian\n",
      "Processing: 16-talk.politics.guns\n",
      "Processing: 17-talk.politics.mideast\n",
      "Processing: 18-talk.politics.misc\n",
      "Processing: 19-talk.religion.misc\n",
      "Processing: 0-alt.atheism\n",
      "Processing: 1-comp.graphics\n",
      "Processing: 2-comp.os.ms-windows.misc\n",
      "Processing: 3-comp.sys.ibm.pc.hardware\n",
      "Processing: 4-comp.sys.mac.hardware\n",
      "Processing: 5-comp.windows.x\n",
      "Processing: 6-misc.forsale\n",
      "Processing: 7-rec.autos\n",
      "Processing: 8-rec.motorcycles\n",
      "Processing: 9-rec.sport.baseball\n",
      "Processing: 10-rec.sport.hockey\n",
      "Processing: 11-sci.crypt\n",
      "Processing: 12-sci.electronics\n",
      "Processing: 13-sci.med\n",
      "Processing: 14-sci.space\n",
      "Processing: 15-soc.religion.christian\n",
      "Processing: 16-talk.politics.guns\n",
      "Processing: 17-talk.politics.mideast\n",
      "Processing: 18-talk.politics.misc\n",
      "Processing: 19-talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "gen_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c73672f7-41e0-4d5c-89d2-bbac5ca81df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_data('data/w2v/20news-train-raw.txt', 'data/w2v/vocab-raw.txt')\n",
    "encode_data('data/w2v/20news-test-raw.txt', 'data/w2v/vocab-raw.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1aa2d33-c5d3-4788-a26a-3da78cdd673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(data_path):\n",
    "    with open(data_path) as f:\n",
    "        d_lines = f.read().splitlines()\n",
    "        \n",
    "    data = []\n",
    "    labels = []\n",
    "    for line in d_lines:\n",
    "        features = line.split('<fff>')\n",
    "        label, doc_id, sentences = int(features[0]), int(features[1]), features[2:]\n",
    "        \n",
    "        labels.append(label)\n",
    "        \n",
    "        doc_tokens = [] # contain tokens for every sentence in the doc\n",
    "        for sent in sentences:\n",
    "            sent_tokens = [int(token) for token in sent.split()]\n",
    "            doc_tokens.append(sent_tokens)\n",
    "            \n",
    "        data.append(doc_tokens)\n",
    "        \n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3265d5ae-19b0-4ba4-92fa-fa9364478408",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = read_dataset('data/w2v/20news-train-encoded.txt')\n",
    "X_test, y_test = read_dataset('data/w2v/20news-test-encoded.txt')\n",
    "\n",
    "y_train = pd.get_dummies(pd.Series(y_train)).values\n",
    "y_test = pd.get_dummies(pd.Series(y_test)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7ddb434-a071-4745-8680-e9a9e286249a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 50, 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c41c9059-e279-4c6d-9ed3-374aa4cb0f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7532, 50, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb171015-d3dd-46ce-b7bb-c6c40ebcaf72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7532, 20)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5ee1705-110b-409a-9943-4f90bbc5dce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c855a68-9438-4c7c-b00f-c3c70414c7c7",
   "metadata": {},
   "source": [
    "# 2. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27caf41e-7a40-4fde-bbca-6f6b37106d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7894a1ff-b31f-43b4-bcab-7ca0d4df522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWithContext(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "437b056a-a150-4e9d-9650-4118d3314f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/w2v/vocab-raw.txt') as f:\n",
    "    vocab_size = len(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9974f456-3623-4230-9bb5-139715ce6570",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    input_dim=vocab_size+2, output_dim=300, input_length=MAX_WORDS_PER_SENTENCE\n",
    ")\n",
    "\n",
    "word_input = Input(shape=(MAX_WORDS_PER_SENTENCE, ), dtype='int32')\n",
    "word_sequence = embedding_layer(word_input)\n",
    "word_lstm = Bidirectional(LSTM(units=100, return_sequences=True))(word_sequence)\n",
    "word_dense = TimeDistributed(Dense(200))(word_lstm)\n",
    "word_att = AttentionWithContext()(word_dense)\n",
    "wordEncoder = Model(word_input, word_att)\n",
    "\n",
    "sent_input = Input(\n",
    "    shape=(MAX_SENTENCES, MAX_WORDS_PER_SENTENCE), dtype='int32')\n",
    "sent_encoder = TimeDistributed(wordEncoder)(sent_input)\n",
    "sent_lstm = Bidirectional(LSTM(100, return_sequences=True))(sent_encoder)\n",
    "sent_dense = TimeDistributed(Dense(200))(sent_lstm)\n",
    "sent_att = AttentionWithContext()(sent_dense)\n",
    "preds = Dense(20, activation='softmax')(sent_att)\n",
    "model = Model(sent_input, preds)\n",
    "\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a489de0-6319-4f62-ad64-faafed8c5892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 50, 50)]          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 50, 200)           6098400   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 200)           240800    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 50, 200)           40200     \n",
      "_________________________________________________________________\n",
      "attention_with_context_1 (At (None, 200)               40400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                4020      \n",
      "=================================================================\n",
      "Total params: 6,423,820\n",
      "Trainable params: 6,423,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a655ee-63dc-4dd9-be4e-5365dc747ce3",
   "metadata": {},
   "source": [
    "__Since I can't run it on my local machine, see the attetion_v2 for the result__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fa0528-f1b2-4d9b-bccf-8c9296ddd9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, validation_split=0.1,\n",
    "          epochs=1, batch_size=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
