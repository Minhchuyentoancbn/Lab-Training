{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-10-28T12:44:46.655332Z",
     "iopub.status.busy": "2022-10-28T12:44:46.654018Z",
     "iopub.status.idle": "2022-10-28T12:44:46.662118Z",
     "shell.execute_reply": "2022-10-28T12:44:46.661154Z",
     "shell.execute_reply.started": "2022-10-28T12:44:46.655293Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import tensorflow.keras.initializers as initializers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "import tensorflow.keras.constraints as constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T12:44:46.664154Z",
     "iopub.status.busy": "2022-10-28T12:44:46.663476Z",
     "iopub.status.idle": "2022-10-28T12:44:46.682606Z",
     "shell.execute_reply": "2022-10-28T12:44:46.681689Z",
     "shell.execute_reply.started": "2022-10-28T12:44:46.664112Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCES = 50\n",
    "MAX_WORDS_PER_SENTENCE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T12:48:05.930965Z",
     "iopub.status.busy": "2022-10-28T12:48:05.930461Z",
     "iopub.status.idle": "2022-10-28T12:48:05.939634Z",
     "shell.execute_reply": "2022-10-28T12:48:05.938601Z",
     "shell.execute_reply.started": "2022-10-28T12:48:05.930932Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_dataset(data_path):\n",
    "    with open(data_path) as f:\n",
    "        d_lines = f.read().splitlines()\n",
    "        \n",
    "    data = []\n",
    "    labels = []\n",
    "    for line in d_lines:\n",
    "        features = line.split('<fff>')\n",
    "        label, doc_id, sentences = int(features[0]), int(features[1]), features[2:]\n",
    "        \n",
    "        labels.append(label)\n",
    "        \n",
    "        doc_tokens = [] # contain tokens for every sentence in the doc\n",
    "        for sent in sentences:\n",
    "            sent_tokens = [int(token) for token in sent.split()]\n",
    "            doc_tokens.append(sent_tokens)\n",
    "            \n",
    "        data.append(doc_tokens)\n",
    "        \n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T12:48:55.579197Z",
     "iopub.status.busy": "2022-10-28T12:48:55.578839Z",
     "iopub.status.idle": "2022-10-28T12:49:13.410160Z",
     "shell.execute_reply": "2022-10-28T12:49:13.409189Z",
     "shell.execute_reply.started": "2022-10-28T12:48:55.579166Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = read_dataset('../input/attention-v2/20news-train-encoded.txt')\n",
    "X_test, y_test = read_dataset('../input/attention-v2/20news-test-encoded.txt')\n",
    "\n",
    "y_train = pd.get_dummies(pd.Series(y_train)).values\n",
    "y_test = pd.get_dummies(pd.Series(y_test)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T12:49:16.721011Z",
     "iopub.status.busy": "2022-10-28T12:49:16.720665Z",
     "iopub.status.idle": "2022-10-28T12:49:16.726774Z",
     "shell.execute_reply": "2022-10-28T12:49:16.725802Z",
     "shell.execute_reply.started": "2022-10-28T12:49:16.720982Z"
    }
   },
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T12:49:39.120670Z",
     "iopub.status.busy": "2022-10-28T12:49:39.120280Z",
     "iopub.status.idle": "2022-10-28T12:49:39.136944Z",
     "shell.execute_reply": "2022-10-28T12:49:39.135611Z",
     "shell.execute_reply.started": "2022-10-28T12:49:39.120638Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttentionWithContext(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T12:51:58.097095Z",
     "iopub.status.busy": "2022-10-28T12:51:58.096407Z",
     "iopub.status.idle": "2022-10-28T12:51:58.103749Z",
     "shell.execute_reply": "2022-10-28T12:51:58.102748Z",
     "shell.execute_reply.started": "2022-10-28T12:51:58.097058Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../input/attention-set/vocab-raw.txt', 'rb') as f:\n",
    "    vocab_size = len(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T12:54:15.919011Z",
     "iopub.status.busy": "2022-10-28T12:54:15.918653Z",
     "iopub.status.idle": "2022-10-28T12:54:18.181851Z",
     "shell.execute_reply": "2022-10-28T12:54:18.180861Z",
     "shell.execute_reply.started": "2022-10-28T12:54:15.918980Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    input_dim=vocab_size+2, output_dim=300, input_length=MAX_WORDS_PER_SENTENCE\n",
    ")\n",
    "\n",
    "word_input = Input(shape=(MAX_WORDS_PER_SENTENCE, ), dtype='int32')\n",
    "word_sequence = embedding_layer(word_input)\n",
    "word_lstm = Bidirectional(LSTM(units=100, return_sequences=True))(word_sequence)\n",
    "word_dense = TimeDistributed(Dense(200))(word_lstm)\n",
    "word_att = AttentionWithContext()(word_dense)\n",
    "wordEncoder = Model(word_input, word_att)\n",
    "\n",
    "sent_input = Input(\n",
    "    shape=(MAX_SENTENCES, MAX_WORDS_PER_SENTENCE), dtype='int32')\n",
    "sent_encoder = TimeDistributed(wordEncoder)(sent_input)\n",
    "sent_lstm = Bidirectional(LSTM(100, return_sequences=True))(sent_encoder)\n",
    "sent_dense = TimeDistributed(Dense(200))(sent_lstm)\n",
    "sent_att = AttentionWithContext()(sent_dense)\n",
    "preds = Dense(20, activation='softmax')(sent_att)\n",
    "model = Model(sent_input, preds)\n",
    "\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T12:54:21.541409Z",
     "iopub.status.busy": "2022-10-28T12:54:21.540404Z",
     "iopub.status.idle": "2022-10-28T12:58:18.563792Z",
     "shell.execute_reply": "2022-10-28T12:58:18.562879Z",
     "shell.execute_reply.started": "2022-10-28T12:54:21.541360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "102/102 [==============================] - 30s 240ms/step - loss: 2.6236 - acc: 0.1170 - val_loss: 6.9210 - val_acc: 0.0654\n",
      "Epoch 2/10\n",
      "102/102 [==============================] - 23s 225ms/step - loss: 1.5084 - acc: 0.4394 - val_loss: 7.8405 - val_acc: 0.1890\n",
      "Epoch 3/10\n",
      "102/102 [==============================] - 23s 225ms/step - loss: 0.6280 - acc: 0.7816 - val_loss: 7.8750 - val_acc: 0.2014\n",
      "Epoch 4/10\n",
      "102/102 [==============================] - 23s 225ms/step - loss: 0.2018 - acc: 0.9392 - val_loss: 8.1863 - val_acc: 0.2173\n",
      "Epoch 5/10\n",
      "102/102 [==============================] - 23s 226ms/step - loss: 0.0857 - acc: 0.9748 - val_loss: 8.9849 - val_acc: 0.2226\n",
      "Epoch 6/10\n",
      "102/102 [==============================] - 23s 226ms/step - loss: 0.0517 - acc: 0.9856 - val_loss: 10.1406 - val_acc: 0.1970\n",
      "Epoch 7/10\n",
      "102/102 [==============================] - 23s 226ms/step - loss: 0.0259 - acc: 0.9927 - val_loss: 10.1168 - val_acc: 0.2111\n",
      "Epoch 8/10\n",
      "102/102 [==============================] - 23s 226ms/step - loss: 0.0124 - acc: 0.9969 - val_loss: 10.1860 - val_acc: 0.2014\n",
      "Epoch 9/10\n",
      "102/102 [==============================] - 23s 224ms/step - loss: 0.0114 - acc: 0.9972 - val_loss: 10.9589 - val_acc: 0.2014\n",
      "Epoch 10/10\n",
      "102/102 [==============================] - 23s 223ms/step - loss: 0.0166 - acc: 0.9953 - val_loss: 9.8147 - val_acc: 0.2164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdbe0a7e050>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_split=0.1,\n",
    "          epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T12:58:18.567234Z",
     "iopub.status.busy": "2022-10-28T12:58:18.566942Z",
     "iopub.status.idle": "2022-10-28T12:58:25.271435Z",
     "shell.execute_reply": "2022-10-28T12:58:25.270528Z",
     "shell.execute_reply.started": "2022-10-28T12:58:18.567208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 6s 27ms/step - loss: 2.1654 - acc: 0.7195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.1654274463653564, 0.7194636464118958]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
