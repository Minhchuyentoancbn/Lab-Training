{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f53ad6e-c3b7-4617-8574-a012bace83cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 1. Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0acff51-fa8a-4e5a-b003-49f7a5f66be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\anacoda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddda2c2e-28c0-470a-b21c-2785a368010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c306b8-7554-401f-8e98-3d9f75d15720",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.constant(5)\n",
    "x2 = tf.constant(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2b812d0-390d-428c-83eb-aa6e631ae5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mul:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = tf.multiply(x1, x2)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7337eea5-f4c6-4cf2-92e8-13a96338bedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mul_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 * x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f794d68d-dce0-4387-8af2-82d7873049a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mul:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    output = sess.run(result)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498e5f1f-909c-4ea3-af72-3d53d4542a82",
   "metadata": {},
   "source": [
    "# 2. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e821c53a-4499-4b77-872e-ec885c4c5101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        self._vocab_size = vocab_size\n",
    "        self._hidden_size = hidden_size\n",
    "    \n",
    "    \n",
    "    def build_graph(self):\n",
    "        self._X = tf.placeholder(tf.float32, shape=[None, self._vocab_size])\n",
    "        self._real_Y = tf.placeholder(tf.int32, shape=[None,])\n",
    "        \n",
    "        weights_1 = tf.get_variable(\n",
    "            name='weights_input_hidden',\n",
    "            shape=(self._vocab_size, self._hidden_size),\n",
    "            initializer=tf.random_normal_initializer(seed=2022)\n",
    "        )\n",
    "        biases_1 = tf.get_variable(\n",
    "            name='biases_input_hidden',\n",
    "            shape=(self._hidden_size),\n",
    "            initializer=tf.random_normal_initializer(seed=2022)\n",
    "        )\n",
    "        weights_2 = tf.get_variable(\n",
    "            name='weights_hidden_output',\n",
    "            shape=(self._hidden_size, NUM_CLASSES),\n",
    "            initializer=tf.random_normal_initializer(seed=2022)\n",
    "        )\n",
    "        biases_2 = tf.get_variable(\n",
    "            name='weight_hidden_output',\n",
    "            shape=(NUM_CLASSES),\n",
    "            initializer=tf.random_normal_initializer(seed=2022)\n",
    "        )\n",
    "        hidden = tf.matmul(self._X, weights_1) + biases_1\n",
    "        hidden = tf.sigmoid(hidden)\n",
    "        logits = tf.matmul(hidden, weights_2) + biases_2\n",
    "        \n",
    "        labels_one_hot = tf.one_hot(indices=self._real_Y, depth=NUM_CLASSES,\n",
    "                                    dtype=tf.float32)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot,\n",
    "                                                       logits=logits)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        probs = tf.nn.softmax(logits)\n",
    "        predicted_labels = tf.argmax(probs, axis=1)\n",
    "        predicted_labels = tf.squeeze(predicted_labels)\n",
    "        \n",
    "        return predicted_labels, loss\n",
    "        \n",
    "    \n",
    "    \n",
    "    def trainer(self, loss, learning_rate):\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d0927ae-00af-469a-aacb-95c030e8743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_data_reader = DataReader(\n",
    "        data_path='../Session1/data/20news-bydate/tfidf_train.txt',\n",
    "        batch_size=50,\n",
    "        vocab_size=vocab_size\n",
    "    )\n",
    "    test_data_reader = DataReader(\n",
    "        data_path='../Session1/data/20news-bydate/tfidf_test.txt',\n",
    "        batch_size=50,\n",
    "        vocab_size=vocab_size\n",
    "    )\n",
    "    return train_data_reader, test_data_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de7cd789-4c75-43a7-a1ba-c81f1f774cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "    def __init__(self, data_path, batch_size, vocab_size):\n",
    "        self._batch_size = batch_size\n",
    "        with open(data_path) as f:\n",
    "            d_lines = f.read().splitlines()\n",
    "            \n",
    "        self._data = []\n",
    "        self._labels = []\n",
    "        for data_id, line in enumerate(d_lines):\n",
    "            vector = [0.0 for _ in range(vocab_size)]\n",
    "            features = line.split('<fff>')\n",
    "            label, doc_id = int(features[0]), int(features[1])\n",
    "            tokens = features[2].split()\n",
    "            for token in tokens:\n",
    "                index, value = int(token.split(':')[0]), float(token.split(':')[1])\n",
    "                vector[index] = value\n",
    "\n",
    "            self._data.append(vector)\n",
    "            self._labels.append(label)\n",
    "        \n",
    "        self._data = np.array(self._data)\n",
    "        self._labels = np.array(self._labels)\n",
    "        \n",
    "        self._num_epoch = 0\n",
    "        self._batch_id = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        start = self._batch_id * self._batch_size\n",
    "        end = start + self._batch_size\n",
    "        self._batch_id += 1\n",
    "        \n",
    "        if end + self._batch_size > len(self._data):\n",
    "            end = len(self._data)\n",
    "            self._num_epoch += 1\n",
    "            self._batch_id = 0\n",
    "            indices = list(range(len(self._data)))\n",
    "            random.seed(2022)\n",
    "            random.shuffle(indices)\n",
    "            self._data, self._labels = self._data[indices], self._labels[indices]\n",
    "        \n",
    "        return self._data[start:end], self._labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cd6132e-8eb8-42ca-9be6-6ad0b8b5b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parameters(name, value, epoch):\n",
    "    filename = name.replace(':', '-colon-') + f'-epoch-{epoch}.txt'\n",
    "    if len(value.shape) == 1:\n",
    "        string_form = ','.join([str(number) for number in value])\n",
    "    else:\n",
    "        string_form = '\\n'.join([','.join([str(number) for number in value[row]]) \n",
    "                                 for row in range(value.shape[0])])\n",
    "    with open('saved-paras/' + filename, 'w') as f:\n",
    "        f.write(string_form)\n",
    "        \n",
    "        \n",
    "def restore_parameters(name, epoch):\n",
    "    filename = name.replace(':', '-colon-') + f'-epoch-{epoch}.txt'\n",
    "    with open('saved-paras/' + filename) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    if len(lines) == 1:\n",
    "        value = [float(number) for number in lines[0].split(',')]\n",
    "    else:\n",
    "        value = [[float(number) for number in lines[row].split(',')]\n",
    "                 for row in range(len(lines))]\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a3caef1-9aa0-4359-ba7d-c4bb5821dc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\anacoda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "step: 1, loss: 10.310537338256836\n",
      "step: 2, loss: 1.3004164695739746\n",
      "step: 3, loss: 0.0016403989866375923\n",
      "step: 4, loss: 3.516587867125054e-06\n",
      "step: 5, loss: 4.5299501039153256e-08\n",
      "step: 6, loss: 0.0\n",
      "step: 7, loss: 0.0\n",
      "step: 8, loss: 0.0\n",
      "step: 9, loss: 0.0\n",
      "step: 10, loss: 14.509949684143066\n",
      "step: 11, loss: 34.158931732177734\n",
      "step: 12, loss: 30.411937713623047\n",
      "step: 13, loss: 25.305627822875977\n",
      "step: 14, loss: 19.19487762451172\n",
      "step: 15, loss: 12.727575302124023\n",
      "step: 16, loss: 6.510400295257568\n",
      "step: 17, loss: 1.7971683740615845\n",
      "step: 18, loss: 0.29119715094566345\n",
      "step: 19, loss: 0.01314794085919857\n",
      "step: 20, loss: 2.5943303626263514e-05\n",
      "step: 21, loss: 6.675683721368841e-07\n",
      "step: 22, loss: 20.193723678588867\n",
      "step: 23, loss: 26.604265213012695\n",
      "step: 24, loss: 24.030977249145508\n",
      "step: 25, loss: 21.514698028564453\n",
      "step: 26, loss: 18.254606246948242\n",
      "step: 27, loss: 14.923528671264648\n",
      "step: 28, loss: 11.781656265258789\n",
      "step: 29, loss: 9.13598918914795\n",
      "step: 30, loss: 6.169145107269287\n",
      "step: 31, loss: 3.7692553997039795\n",
      "step: 32, loss: 1.5504240989685059\n",
      "step: 33, loss: 0.4978565275669098\n",
      "step: 34, loss: 7.774329662322998\n",
      "step: 35, loss: 7.910475254058838\n",
      "step: 36, loss: 7.602890491485596\n",
      "step: 37, loss: 6.995984077453613\n",
      "step: 38, loss: 5.800809860229492\n",
      "step: 39, loss: 4.073175430297852\n",
      "step: 40, loss: 2.825237512588501\n",
      "step: 41, loss: 1.6746766567230225\n",
      "step: 42, loss: 1.051925539970398\n",
      "step: 43, loss: 0.47288772463798523\n",
      "step: 44, loss: 0.1873326450586319\n",
      "step: 45, loss: 1.2989819049835205\n",
      "step: 46, loss: 12.056334495544434\n",
      "step: 47, loss: 11.776042938232422\n",
      "step: 48, loss: 11.067693710327148\n",
      "step: 49, loss: 10.576421737670898\n",
      "step: 50, loss: 9.628705978393555\n",
      "step: 51, loss: 8.54720687866211\n",
      "step: 52, loss: 7.321150302886963\n",
      "step: 53, loss: 6.276647567749023\n",
      "step: 54, loss: 5.026681900024414\n",
      "step: 55, loss: 4.775671005249023\n",
      "step: 56, loss: 3.797698736190796\n",
      "step: 57, loss: 7.500406265258789\n",
      "step: 58, loss: 10.458182334899902\n",
      "step: 59, loss: 10.185178756713867\n",
      "step: 60, loss: 9.784021377563477\n",
      "step: 61, loss: 8.756321907043457\n",
      "step: 62, loss: 8.117875099182129\n",
      "step: 63, loss: 7.42098331451416\n",
      "step: 64, loss: 6.667201519012451\n",
      "step: 65, loss: 6.158268928527832\n",
      "step: 66, loss: 5.660622596740723\n",
      "step: 67, loss: 5.079156398773193\n",
      "step: 68, loss: 4.281686305999756\n",
      "step: 69, loss: 7.178234100341797\n",
      "step: 70, loss: 7.893652439117432\n",
      "step: 71, loss: 7.744872570037842\n",
      "step: 72, loss: 7.1701788902282715\n",
      "step: 73, loss: 6.645425796508789\n",
      "step: 74, loss: 5.940722465515137\n",
      "step: 75, loss: 5.617436408996582\n",
      "step: 76, loss: 5.297227382659912\n",
      "step: 77, loss: 4.836315155029297\n",
      "step: 78, loss: 4.585159778594971\n",
      "step: 79, loss: 4.204850673675537\n",
      "step: 80, loss: 3.8549389839172363\n",
      "step: 81, loss: 6.818647861480713\n",
      "step: 82, loss: 6.702044486999512\n",
      "step: 83, loss: 6.372963905334473\n",
      "step: 84, loss: 6.1338982582092285\n",
      "step: 85, loss: 5.729230880737305\n",
      "step: 86, loss: 5.462025165557861\n",
      "step: 87, loss: 5.1784796714782715\n",
      "step: 88, loss: 4.7568559646606445\n",
      "step: 89, loss: 4.504138946533203\n",
      "step: 90, loss: 3.9625632762908936\n",
      "step: 91, loss: 3.6850576400756836\n",
      "step: 92, loss: 3.5031397342681885\n",
      "step: 93, loss: 8.623797416687012\n",
      "step: 94, loss: 8.42919635772705\n",
      "step: 95, loss: 8.060823440551758\n",
      "step: 96, loss: 7.632627487182617\n",
      "step: 97, loss: 7.307896614074707\n",
      "step: 98, loss: 6.8398356437683105\n",
      "step: 99, loss: 6.547019004821777\n",
      "step: 100, loss: 6.106749057769775\n",
      "step: 101, loss: 5.741650581359863\n",
      "step: 102, loss: 5.433419704437256\n",
      "step: 103, loss: 4.837959289550781\n",
      "step: 104, loss: 5.037753105163574\n",
      "step: 105, loss: 8.947265625\n",
      "step: 106, loss: 8.855538368225098\n",
      "step: 107, loss: 8.230902671813965\n",
      "step: 108, loss: 7.966585636138916\n",
      "step: 109, loss: 7.348860263824463\n",
      "step: 110, loss: 7.055999279022217\n",
      "step: 111, loss: 6.615610122680664\n",
      "step: 112, loss: 6.214686870574951\n",
      "step: 113, loss: 5.709089279174805\n",
      "step: 114, loss: 5.410078525543213\n",
      "step: 115, loss: 4.841432571411133\n",
      "step: 116, loss: 5.0283589363098145\n",
      "step: 117, loss: 8.593684196472168\n",
      "step: 118, loss: 8.227675437927246\n",
      "step: 119, loss: 7.786591053009033\n",
      "step: 120, loss: 7.700339317321777\n",
      "step: 121, loss: 7.441349506378174\n",
      "step: 122, loss: 6.848970890045166\n",
      "step: 123, loss: 6.439664363861084\n",
      "step: 124, loss: 6.181361675262451\n",
      "step: 125, loss: 5.896735191345215\n",
      "step: 126, loss: 5.510334491729736\n",
      "step: 127, loss: 5.319511890411377\n",
      "step: 128, loss: 5.306002140045166\n",
      "step: 129, loss: 6.109546661376953\n",
      "step: 130, loss: 5.772021293640137\n",
      "step: 131, loss: 5.583859920501709\n",
      "step: 132, loss: 5.2731170654296875\n",
      "step: 133, loss: 4.976511001586914\n",
      "step: 134, loss: 4.619255542755127\n",
      "step: 135, loss: 4.23189640045166\n",
      "step: 136, loss: 4.026198863983154\n",
      "step: 137, loss: 3.4676263332366943\n",
      "step: 138, loss: 3.0632388591766357\n",
      "step: 139, loss: 2.518244743347168\n",
      "step: 140, loss: 3.3708853721618652\n",
      "step: 141, loss: 6.372512340545654\n",
      "step: 142, loss: 6.383303165435791\n",
      "step: 143, loss: 5.973599910736084\n",
      "step: 144, loss: 5.617959976196289\n",
      "step: 145, loss: 5.495891571044922\n",
      "step: 146, loss: 5.096148490905762\n",
      "step: 147, loss: 4.882628440856934\n",
      "step: 148, loss: 4.53962516784668\n",
      "step: 149, loss: 4.420019149780273\n",
      "step: 150, loss: 3.977400302886963\n",
      "step: 151, loss: 3.7598679065704346\n",
      "step: 152, loss: 5.7548980712890625\n",
      "step: 153, loss: 7.807887077331543\n",
      "step: 154, loss: 7.398252487182617\n",
      "step: 155, loss: 7.307184219360352\n",
      "step: 156, loss: 6.977217197418213\n",
      "step: 157, loss: 6.588630199432373\n",
      "step: 158, loss: 6.3523783683776855\n",
      "step: 159, loss: 6.062891006469727\n",
      "step: 160, loss: 5.728707790374756\n",
      "step: 161, loss: 5.4091715812683105\n",
      "step: 162, loss: 5.141292572021484\n",
      "step: 163, loss: 4.789690017700195\n",
      "step: 164, loss: 5.97927188873291\n",
      "step: 165, loss: 6.9974541664123535\n",
      "step: 166, loss: 6.7317304611206055\n",
      "step: 167, loss: 6.5149149894714355\n",
      "step: 168, loss: 6.223431587219238\n",
      "step: 169, loss: 6.09788703918457\n",
      "step: 170, loss: 5.643681526184082\n",
      "step: 171, loss: 5.337114334106445\n",
      "step: 172, loss: 5.098021030426025\n",
      "step: 173, loss: 4.736550807952881\n",
      "step: 174, loss: 4.425818920135498\n",
      "step: 175, loss: 4.084048271179199\n",
      "step: 176, loss: 8.648624420166016\n",
      "step: 177, loss: 9.539924621582031\n",
      "step: 178, loss: 8.762360572814941\n",
      "step: 179, loss: 8.396915435791016\n",
      "step: 180, loss: 8.11834716796875\n",
      "step: 181, loss: 7.756017684936523\n",
      "step: 182, loss: 7.396204948425293\n",
      "step: 183, loss: 7.121826171875\n",
      "step: 184, loss: 6.7890448570251465\n",
      "step: 185, loss: 6.482443809509277\n",
      "step: 186, loss: 6.160430431365967\n",
      "step: 187, loss: 5.819538593292236\n",
      "step: 188, loss: 9.229475975036621\n",
      "step: 189, loss: 9.902752876281738\n",
      "step: 190, loss: 9.74706745147705\n",
      "step: 191, loss: 9.38791275024414\n",
      "step: 192, loss: 9.140660285949707\n",
      "step: 193, loss: 8.822193145751953\n",
      "step: 194, loss: 8.568774223327637\n",
      "step: 195, loss: 8.198599815368652\n",
      "step: 196, loss: 7.890231132507324\n",
      "step: 197, loss: 7.573208808898926\n",
      "step: 198, loss: 7.240767002105713\n",
      "step: 199, loss: 7.579476356506348\n",
      "step: 200, loss: 7.5256571769714355\n",
      "step: 201, loss: 7.313941478729248\n",
      "step: 202, loss: 7.03769063949585\n",
      "step: 203, loss: 6.745699882507324\n",
      "step: 204, loss: 6.4277496337890625\n",
      "step: 205, loss: 6.115415096282959\n",
      "step: 206, loss: 5.731350898742676\n",
      "step: 207, loss: 5.2503662109375\n",
      "step: 208, loss: 4.770415782928467\n",
      "step: 209, loss: 4.284518718719482\n",
      "step: 210, loss: 7.388688564300537\n",
      "step: 211, loss: 10.245038032531738\n",
      "step: 212, loss: 10.17570972442627\n",
      "step: 213, loss: 9.66708755493164\n",
      "step: 214, loss: 9.551701545715332\n",
      "step: 215, loss: 9.003417015075684\n",
      "step: 216, loss: 8.643843650817871\n",
      "step: 217, loss: 8.308321952819824\n",
      "step: 218, loss: 7.824355602264404\n",
      "step: 219, loss: 6.786439418792725\n",
      "step: 220, loss: 5.971633434295654\n",
      "step: 221, loss: 5.586926460266113\n",
      "step: 222, loss: 5.022733688354492\n",
      "step: 223, loss: 4.461597442626953\n",
      "step: 224, loss: 4.039645195007324\n",
      "step: 225, loss: 3.5557668209075928\n",
      "step: 226, loss: 2.786071300506592\n",
      "step: 227, loss: 2.6046433448791504\n",
      "step: 228, loss: 2.5107672214508057\n",
      "step: 229, loss: 2.836989402770996\n",
      "step: 230, loss: 2.781384229660034\n",
      "step: 231, loss: 2.5835955142974854\n",
      "step: 232, loss: 2.456650733947754\n",
      "step: 233, loss: 2.68058705329895\n",
      "step: 234, loss: 2.6062192916870117\n",
      "step: 235, loss: 2.7512946128845215\n",
      "step: 236, loss: 2.5217013359069824\n",
      "step: 237, loss: 2.7545909881591797\n",
      "step: 238, loss: 2.4238946437835693\n",
      "step: 239, loss: 2.432324171066284\n",
      "step: 240, loss: 2.443131446838379\n",
      "step: 241, loss: 2.6221060752868652\n",
      "step: 242, loss: 2.3603177070617676\n",
      "step: 243, loss: 2.378842353820801\n",
      "step: 244, loss: 2.4427173137664795\n",
      "step: 245, loss: 2.604391574859619\n",
      "step: 246, loss: 2.051759958267212\n",
      "step: 247, loss: 1.939509391784668\n",
      "step: 248, loss: 2.2424588203430176\n",
      "step: 249, loss: 2.1461880207061768\n",
      "step: 250, loss: 2.0291101932525635\n",
      "step: 251, loss: 1.4187631607055664\n",
      "step: 252, loss: 2.2425925731658936\n",
      "step: 253, loss: 1.731231689453125\n",
      "step: 254, loss: 2.023829698562622\n",
      "step: 255, loss: 1.7310513257980347\n",
      "step: 256, loss: 1.8188493251800537\n",
      "step: 257, loss: 1.8014700412750244\n",
      "step: 258, loss: 1.8039352893829346\n",
      "step: 259, loss: 1.905392050743103\n",
      "step: 260, loss: 1.939347505569458\n",
      "step: 261, loss: 1.4195420742034912\n",
      "step: 262, loss: 1.510685682296753\n",
      "step: 263, loss: 1.4993547201156616\n",
      "step: 264, loss: 1.605903148651123\n",
      "step: 265, loss: 1.4982681274414062\n",
      "step: 266, loss: 1.5456575155258179\n",
      "step: 267, loss: 1.7465314865112305\n",
      "step: 268, loss: 1.5291332006454468\n",
      "step: 269, loss: 1.568549633026123\n",
      "step: 270, loss: 1.855098843574524\n",
      "step: 271, loss: 1.70293390750885\n",
      "step: 272, loss: 1.0941989421844482\n",
      "step: 273, loss: 1.7735449075698853\n",
      "step: 274, loss: 1.3131452798843384\n",
      "step: 275, loss: 1.2850838899612427\n",
      "step: 276, loss: 1.4797853231430054\n",
      "step: 277, loss: 1.5073668956756592\n",
      "step: 278, loss: 1.2325118780136108\n",
      "step: 279, loss: 1.476479172706604\n",
      "step: 280, loss: 1.3309575319290161\n",
      "step: 281, loss: 1.1395673751831055\n",
      "step: 282, loss: 1.3180298805236816\n",
      "step: 283, loss: 1.3378322124481201\n",
      "step: 284, loss: 1.2434536218643188\n",
      "step: 285, loss: 1.1978355646133423\n",
      "step: 286, loss: 1.4081534147262573\n",
      "step: 287, loss: 1.1413064002990723\n",
      "step: 288, loss: 1.2116858959197998\n",
      "step: 289, loss: 1.354746699333191\n",
      "step: 290, loss: 1.4274505376815796\n",
      "step: 291, loss: 1.1654081344604492\n",
      "step: 292, loss: 1.3882275819778442\n",
      "step: 293, loss: 1.094618320465088\n",
      "step: 294, loss: 1.5022157430648804\n",
      "step: 295, loss: 1.0822277069091797\n",
      "step: 296, loss: 1.3263195753097534\n",
      "step: 297, loss: 1.1203677654266357\n",
      "step: 298, loss: 1.155405879020691\n",
      "step: 299, loss: 0.8984587788581848\n",
      "step: 300, loss: 1.0603423118591309\n",
      "step: 301, loss: 1.30851411819458\n",
      "step: 302, loss: 1.4772547483444214\n",
      "step: 303, loss: 0.9140425324440002\n",
      "step: 304, loss: 0.9806209802627563\n",
      "step: 305, loss: 0.9174291491508484\n",
      "step: 306, loss: 0.9802941679954529\n",
      "step: 307, loss: 0.9942395687103271\n",
      "step: 308, loss: 1.2567662000656128\n",
      "step: 309, loss: 1.2808279991149902\n",
      "step: 310, loss: 0.8634152412414551\n",
      "step: 311, loss: 1.0356744527816772\n",
      "step: 312, loss: 0.7692931890487671\n",
      "step: 313, loss: 0.6914204955101013\n",
      "step: 314, loss: 0.9738701581954956\n",
      "step: 315, loss: 0.7587761878967285\n",
      "step: 316, loss: 1.061195731163025\n",
      "step: 317, loss: 0.7147750854492188\n",
      "step: 318, loss: 0.9628905653953552\n",
      "step: 319, loss: 0.9014543890953064\n",
      "step: 320, loss: 1.049963355064392\n",
      "step: 321, loss: 0.8985351324081421\n",
      "step: 322, loss: 0.7582231163978577\n",
      "step: 323, loss: 0.9523909687995911\n",
      "step: 324, loss: 0.6930081844329834\n",
      "step: 325, loss: 0.8634243607521057\n",
      "step: 326, loss: 0.9545206427574158\n",
      "step: 327, loss: 1.0140241384506226\n",
      "step: 328, loss: 0.7724838852882385\n",
      "step: 329, loss: 0.4900471866130829\n",
      "step: 330, loss: 0.7517131567001343\n",
      "step: 331, loss: 0.7924992442131042\n",
      "step: 332, loss: 0.6208745837211609\n",
      "step: 333, loss: 0.8962570428848267\n",
      "step: 334, loss: 0.7112028002738953\n",
      "step: 335, loss: 0.3807530999183655\n",
      "step: 336, loss: 0.8394342064857483\n",
      "step: 337, loss: 0.6781471967697144\n",
      "step: 338, loss: 0.4717746078968048\n",
      "step: 339, loss: 0.4912976026535034\n",
      "step: 340, loss: 0.8665271997451782\n",
      "step: 341, loss: 0.7986534237861633\n",
      "step: 342, loss: 0.7510879039764404\n",
      "step: 343, loss: 0.37354907393455505\n",
      "step: 344, loss: 0.3263286352157593\n",
      "step: 345, loss: 0.6609020233154297\n",
      "step: 346, loss: 0.4405190646648407\n",
      "step: 347, loss: 0.44485899806022644\n",
      "step: 348, loss: 0.4593394994735718\n",
      "step: 349, loss: 0.6019275188446045\n",
      "step: 350, loss: 1.0103874206542969\n",
      "step: 351, loss: 0.7828883528709412\n",
      "step: 352, loss: 0.2672877907752991\n",
      "step: 353, loss: 0.5395292043685913\n",
      "step: 354, loss: 0.9532546401023865\n",
      "step: 355, loss: 0.5375111699104309\n",
      "step: 356, loss: 0.4579841196537018\n",
      "step: 357, loss: 0.6761967539787292\n",
      "step: 358, loss: 0.6054679751396179\n",
      "step: 359, loss: 0.7876479625701904\n",
      "step: 360, loss: 0.4434429109096527\n",
      "step: 361, loss: 0.5698270201683044\n",
      "step: 362, loss: 0.6913310289382935\n",
      "step: 363, loss: 0.43270719051361084\n",
      "step: 364, loss: 0.679190993309021\n",
      "step: 365, loss: 0.5051405429840088\n",
      "step: 366, loss: 0.5134592652320862\n",
      "step: 367, loss: 0.7148206233978271\n",
      "step: 368, loss: 0.457685649394989\n",
      "step: 369, loss: 0.6377715468406677\n",
      "step: 370, loss: 0.979756772518158\n",
      "step: 371, loss: 0.5814562439918518\n",
      "step: 372, loss: 0.39824849367141724\n",
      "step: 373, loss: 0.5106273889541626\n",
      "step: 374, loss: 0.7314918637275696\n",
      "step: 375, loss: 0.4040842056274414\n",
      "step: 376, loss: 0.7413698434829712\n",
      "step: 377, loss: 0.798270046710968\n",
      "step: 378, loss: 0.4282793700695038\n",
      "step: 379, loss: 0.734169065952301\n",
      "step: 380, loss: 0.7617623805999756\n",
      "step: 381, loss: 0.6092198491096497\n",
      "step: 382, loss: 0.5884665846824646\n",
      "step: 383, loss: 0.4729670286178589\n",
      "step: 384, loss: 0.9025822281837463\n",
      "step: 385, loss: 0.6881537437438965\n",
      "step: 386, loss: 0.46079838275909424\n",
      "step: 387, loss: 0.577318012714386\n",
      "step: 388, loss: 0.7665813565254211\n",
      "step: 389, loss: 0.42828163504600525\n",
      "step: 390, loss: 0.8351837396621704\n",
      "step: 391, loss: 0.6446540355682373\n",
      "step: 392, loss: 0.454153448343277\n",
      "step: 393, loss: 0.5424491763114929\n",
      "step: 394, loss: 0.4623710513114929\n",
      "step: 395, loss: 0.4652961790561676\n",
      "step: 396, loss: 0.3346276879310608\n",
      "step: 397, loss: 0.24279887974262238\n",
      "step: 398, loss: 0.2328091412782669\n",
      "step: 399, loss: 0.4743424952030182\n",
      "step: 400, loss: 0.3405283987522125\n",
      "step: 401, loss: 0.5835931301116943\n",
      "step: 402, loss: 0.26372548937797546\n",
      "step: 403, loss: 0.6824483275413513\n",
      "step: 404, loss: 0.1637691855430603\n",
      "step: 405, loss: 0.7323509454727173\n",
      "step: 406, loss: 0.4867302179336548\n",
      "step: 407, loss: 0.593976616859436\n",
      "step: 408, loss: 0.6004481315612793\n",
      "step: 409, loss: 0.9426226615905762\n",
      "step: 410, loss: 1.024135708808899\n",
      "step: 411, loss: 0.40936410427093506\n",
      "step: 412, loss: 0.6780241131782532\n",
      "step: 413, loss: 0.4078422784805298\n",
      "step: 414, loss: 0.320781409740448\n",
      "step: 415, loss: 0.4287486970424652\n",
      "step: 416, loss: 0.22532249987125397\n",
      "step: 417, loss: 0.43133485317230225\n",
      "step: 418, loss: 0.822248101234436\n",
      "step: 419, loss: 0.5080307126045227\n",
      "step: 420, loss: 0.6182029843330383\n",
      "step: 421, loss: 0.42742642760276794\n",
      "step: 422, loss: 0.2639164626598358\n",
      "step: 423, loss: 0.3366658389568329\n",
      "step: 424, loss: 0.392782062292099\n",
      "step: 425, loss: 0.16914254426956177\n",
      "step: 426, loss: 0.21606627106666565\n",
      "step: 427, loss: 0.25213202834129333\n",
      "step: 428, loss: 0.5933122038841248\n",
      "step: 429, loss: 0.37045586109161377\n",
      "step: 430, loss: 0.4588111937046051\n",
      "step: 431, loss: 0.33866459131240845\n",
      "step: 432, loss: 0.15327399969100952\n",
      "step: 433, loss: 0.22331362962722778\n",
      "step: 434, loss: 0.5497484803199768\n",
      "step: 435, loss: 0.49996089935302734\n",
      "step: 436, loss: 0.4944442808628082\n",
      "step: 437, loss: 0.5051600933074951\n",
      "step: 438, loss: 0.34405913949012756\n",
      "step: 439, loss: 0.8803772926330566\n",
      "step: 440, loss: 0.2457616627216339\n",
      "step: 441, loss: 0.17675067484378815\n",
      "step: 442, loss: 0.6027167439460754\n",
      "step: 443, loss: 0.3335062265396118\n",
      "step: 444, loss: 0.5114670395851135\n",
      "step: 445, loss: 0.45953112840652466\n",
      "step: 446, loss: 0.3592939078807831\n",
      "step: 447, loss: 0.4578792452812195\n",
      "step: 448, loss: 0.6343927979469299\n",
      "step: 449, loss: 0.4099174439907074\n",
      "step: 450, loss: 0.6172389388084412\n",
      "step: 451, loss: 0.3062356412410736\n",
      "step: 452, loss: 0.02504144050180912\n",
      "step: 453, loss: 0.11242501437664032\n",
      "step: 454, loss: 0.16364917159080505\n",
      "step: 455, loss: 0.46030449867248535\n",
      "step: 456, loss: 0.28798195719718933\n",
      "step: 457, loss: 0.13427233695983887\n",
      "step: 458, loss: 0.295807421207428\n",
      "step: 459, loss: 0.12795768678188324\n",
      "step: 460, loss: 0.12216932326555252\n",
      "step: 461, loss: 0.0802931860089302\n",
      "step: 462, loss: 0.2388741821050644\n",
      "step: 463, loss: 0.25483590364456177\n",
      "step: 464, loss: 0.09247363358736038\n",
      "step: 465, loss: 0.1654699295759201\n",
      "step: 466, loss: 0.0773378312587738\n",
      "step: 467, loss: 0.069778211414814\n",
      "step: 468, loss: 0.08742768317461014\n",
      "step: 469, loss: 0.443365216255188\n",
      "step: 470, loss: 0.2863937020301819\n",
      "step: 471, loss: 0.3395484685897827\n",
      "step: 472, loss: 0.09356140345335007\n",
      "step: 473, loss: 0.16454149782657623\n",
      "step: 474, loss: 0.13266006112098694\n",
      "step: 475, loss: 0.11867308616638184\n",
      "step: 476, loss: 0.17481908202171326\n",
      "step: 477, loss: 0.14182408154010773\n",
      "step: 478, loss: 0.36980706453323364\n",
      "step: 479, loss: 0.19178344309329987\n",
      "step: 480, loss: 0.21497781574726105\n",
      "step: 481, loss: 0.08147281408309937\n",
      "step: 482, loss: 0.2923138439655304\n",
      "step: 483, loss: 0.1485823541879654\n",
      "step: 484, loss: 0.2825459837913513\n",
      "step: 485, loss: 0.32013511657714844\n",
      "step: 486, loss: 0.3257996737957001\n",
      "step: 487, loss: 0.24185070395469666\n",
      "step: 488, loss: 0.12896595895290375\n",
      "step: 489, loss: 0.19886203110218048\n",
      "step: 490, loss: 0.2805659770965576\n",
      "step: 491, loss: 0.05345669388771057\n",
      "step: 492, loss: 0.08089358359575272\n",
      "step: 493, loss: 0.1167478859424591\n",
      "step: 494, loss: 0.12403274327516556\n",
      "step: 495, loss: 0.2949480414390564\n",
      "step: 496, loss: 0.17946740984916687\n",
      "step: 497, loss: 0.15770189464092255\n",
      "step: 498, loss: 0.12056605517864227\n",
      "step: 499, loss: 0.12972809374332428\n",
      "step: 500, loss: 0.08865796774625778\n",
      "step: 501, loss: 0.36673203110694885\n",
      "step: 502, loss: 0.07768968492746353\n",
      "step: 503, loss: 0.08626849204301834\n",
      "step: 504, loss: 0.20362117886543274\n",
      "step: 505, loss: 0.11693510413169861\n",
      "step: 506, loss: 0.09351985156536102\n",
      "step: 507, loss: 0.14319097995758057\n",
      "step: 508, loss: 0.13545770943164825\n",
      "step: 509, loss: 0.12860937416553497\n",
      "step: 510, loss: 0.13680599629878998\n",
      "step: 511, loss: 0.12251977622509003\n",
      "step: 512, loss: 0.10408295691013336\n",
      "step: 513, loss: 0.3577429950237274\n",
      "step: 514, loss: 0.12435486167669296\n",
      "step: 515, loss: 0.3918464779853821\n",
      "step: 516, loss: 0.21686437726020813\n",
      "step: 517, loss: 0.06175577640533447\n",
      "step: 518, loss: 0.2045353651046753\n",
      "step: 519, loss: 0.1488087922334671\n",
      "step: 520, loss: 0.0757497027516365\n",
      "step: 521, loss: 0.0823080837726593\n",
      "step: 522, loss: 0.17299604415893555\n",
      "step: 523, loss: 0.04920073598623276\n",
      "step: 524, loss: 0.25892964005470276\n",
      "step: 525, loss: 0.22428745031356812\n",
      "step: 526, loss: 0.14413835108280182\n",
      "step: 527, loss: 0.13891354203224182\n",
      "step: 528, loss: 0.23858872056007385\n",
      "step: 529, loss: 0.2987801730632782\n",
      "step: 530, loss: 0.25140380859375\n",
      "step: 531, loss: 0.21485303342342377\n",
      "step: 532, loss: 0.09701642394065857\n",
      "step: 533, loss: 0.22278890013694763\n",
      "step: 534, loss: 0.3538675010204315\n",
      "step: 535, loss: 0.09269216656684875\n",
      "step: 536, loss: 0.2006058245897293\n",
      "step: 537, loss: 0.284305602312088\n",
      "step: 538, loss: 0.08609992265701294\n",
      "step: 539, loss: 0.15185143053531647\n",
      "step: 540, loss: 0.23027758300304413\n",
      "step: 541, loss: 0.10168559849262238\n",
      "step: 542, loss: 0.159540593624115\n",
      "step: 543, loss: 0.1564500778913498\n",
      "step: 544, loss: 0.08718422800302505\n",
      "step: 545, loss: 0.0921892523765564\n",
      "step: 546, loss: 0.3746200501918793\n",
      "step: 547, loss: 0.09129050374031067\n",
      "step: 548, loss: 0.0835365355014801\n",
      "step: 549, loss: 0.027349183335900307\n",
      "step: 550, loss: 0.03565799444913864\n",
      "step: 551, loss: 0.24619027972221375\n",
      "step: 552, loss: 0.12048200517892838\n",
      "step: 553, loss: 0.32954150438308716\n",
      "step: 554, loss: 0.09030342102050781\n",
      "step: 555, loss: 0.06915311515331268\n",
      "step: 556, loss: 0.4188527762889862\n",
      "step: 557, loss: 0.198674276471138\n",
      "step: 558, loss: 0.09755156189203262\n",
      "step: 559, loss: 0.03190022334456444\n",
      "step: 560, loss: 0.10060694068670273\n",
      "step: 561, loss: 0.28034839034080505\n",
      "step: 562, loss: 0.0905250534415245\n",
      "step: 563, loss: 0.30118563771247864\n",
      "step: 564, loss: 0.33520275354385376\n",
      "step: 565, loss: 0.2085915505886078\n",
      "step: 566, loss: 0.14592976868152618\n",
      "step: 567, loss: 0.3054567277431488\n",
      "step: 568, loss: 0.10159730166196823\n",
      "step: 569, loss: 0.2742944657802582\n",
      "step: 570, loss: 0.029505813494324684\n",
      "step: 571, loss: 0.24465610086917877\n",
      "step: 572, loss: 0.1777249127626419\n",
      "step: 573, loss: 0.0870681032538414\n",
      "step: 574, loss: 0.36306649446487427\n",
      "step: 575, loss: 0.1927856206893921\n",
      "step: 576, loss: 0.29951587319374084\n",
      "step: 577, loss: 0.0587858147919178\n",
      "step: 578, loss: 0.08296557515859604\n",
      "step: 579, loss: 0.18293680250644684\n",
      "step: 580, loss: 0.22178806364536285\n",
      "step: 581, loss: 0.192026749253273\n",
      "step: 582, loss: 0.2324109524488449\n",
      "step: 583, loss: 0.24354104697704315\n",
      "step: 584, loss: 0.11256396025419235\n",
      "step: 585, loss: 0.18110647797584534\n",
      "step: 586, loss: 0.1674269735813141\n",
      "step: 587, loss: 0.05732468515634537\n",
      "step: 588, loss: 0.07199996709823608\n",
      "step: 589, loss: 0.27063828706741333\n",
      "step: 590, loss: 0.1978568285703659\n",
      "step: 591, loss: 0.016524041071534157\n",
      "step: 592, loss: 0.06473609060049057\n",
      "step: 593, loss: 0.07520885765552521\n",
      "step: 594, loss: 0.14561793208122253\n",
      "step: 595, loss: 0.027156053110957146\n",
      "step: 596, loss: 0.1751381754875183\n",
      "step: 597, loss: 0.11352831870317459\n",
      "step: 598, loss: 0.37329065799713135\n",
      "step: 599, loss: 0.11131620407104492\n",
      "step: 600, loss: 0.050239648669958115\n",
      "step: 601, loss: 0.05269886925816536\n",
      "step: 602, loss: 0.23581428825855255\n",
      "step: 603, loss: 0.05068085342645645\n",
      "step: 604, loss: 0.14597997069358826\n",
      "step: 605, loss: 0.18645507097244263\n",
      "step: 606, loss: 0.251466304063797\n",
      "step: 607, loss: 0.16568374633789062\n",
      "step: 608, loss: 0.15735787153244019\n",
      "step: 609, loss: 0.2076164036989212\n",
      "step: 610, loss: 0.7535030841827393\n",
      "step: 611, loss: 0.12346469610929489\n",
      "step: 612, loss: 0.04150809720158577\n",
      "step: 613, loss: 0.1433381885290146\n",
      "step: 614, loss: 0.11716756224632263\n",
      "step: 615, loss: 0.14456921815872192\n",
      "step: 616, loss: 0.17524291574954987\n",
      "step: 617, loss: 0.1320255696773529\n",
      "step: 618, loss: 0.21683236956596375\n",
      "step: 619, loss: 0.0853026956319809\n",
      "step: 620, loss: 0.1577029973268509\n",
      "step: 621, loss: 0.14256571233272552\n",
      "step: 622, loss: 0.08692888170480728\n",
      "step: 623, loss: 0.19225890934467316\n",
      "step: 624, loss: 0.13647513091564178\n",
      "step: 625, loss: 0.09159418940544128\n",
      "step: 626, loss: 0.12232726067304611\n",
      "step: 627, loss: 0.05528124421834946\n",
      "step: 628, loss: 0.13253921270370483\n",
      "step: 629, loss: 0.079942487180233\n",
      "step: 630, loss: 0.18707191944122314\n",
      "step: 631, loss: 0.43674954771995544\n",
      "step: 632, loss: 0.2018757462501526\n",
      "step: 633, loss: 0.1949671357870102\n",
      "step: 634, loss: 0.13199976086616516\n",
      "step: 635, loss: 0.09750889986753464\n",
      "step: 636, loss: 0.3023388981819153\n",
      "step: 637, loss: 0.29504838585853577\n",
      "step: 638, loss: 0.11929928511381149\n",
      "step: 639, loss: 0.13619288802146912\n",
      "step: 640, loss: 0.09607169777154922\n",
      "step: 641, loss: 0.27285799384117126\n",
      "step: 642, loss: 0.30924609303474426\n",
      "step: 643, loss: 0.05539290979504585\n",
      "step: 644, loss: 0.25936824083328247\n",
      "step: 645, loss: 0.057926688343286514\n",
      "step: 646, loss: 0.13808493316173553\n",
      "step: 647, loss: 0.34597259759902954\n",
      "step: 648, loss: 0.21491117775440216\n",
      "step: 649, loss: 0.21522775292396545\n",
      "step: 650, loss: 0.40440207719802856\n",
      "step: 651, loss: 0.36464786529541016\n",
      "step: 652, loss: 0.11439922451972961\n",
      "step: 653, loss: 0.16676117479801178\n",
      "step: 654, loss: 0.29495784640312195\n",
      "step: 655, loss: 0.12897445261478424\n",
      "step: 656, loss: 0.1841791868209839\n",
      "step: 657, loss: 0.032114721834659576\n",
      "step: 658, loss: 0.19115741550922394\n",
      "step: 659, loss: 0.1402577906847\n",
      "step: 660, loss: 0.0319444015622139\n",
      "step: 661, loss: 0.10261910408735275\n",
      "step: 662, loss: 0.055290523916482925\n",
      "step: 663, loss: 0.04058431088924408\n",
      "step: 664, loss: 0.27244359254837036\n",
      "step: 665, loss: 0.048862166702747345\n",
      "step: 666, loss: 0.07617654651403427\n",
      "step: 667, loss: 0.18972238898277283\n",
      "step: 668, loss: 0.03440874442458153\n",
      "step: 669, loss: 0.20178918540477753\n",
      "step: 670, loss: 0.06705038249492645\n",
      "step: 671, loss: 0.5271340608596802\n",
      "step: 672, loss: 0.0985131487250328\n",
      "step: 673, loss: 0.07865718007087708\n",
      "step: 674, loss: 0.19094741344451904\n",
      "step: 675, loss: 0.09568732231855392\n",
      "step: 676, loss: 0.31102290749549866\n",
      "step: 677, loss: 0.15259939432144165\n",
      "step: 678, loss: 0.02903011068701744\n",
      "step: 679, loss: 0.007769640069454908\n",
      "step: 680, loss: 0.02171522192656994\n",
      "step: 681, loss: 0.04251322150230408\n",
      "step: 682, loss: 0.01907307654619217\n",
      "step: 683, loss: 0.02705381065607071\n",
      "step: 684, loss: 0.01615194045007229\n",
      "step: 685, loss: 0.018251804634928703\n",
      "step: 686, loss: 0.07481371611356735\n",
      "step: 687, loss: 0.026688871905207634\n",
      "step: 688, loss: 0.03372004255652428\n",
      "step: 689, loss: 0.0074142408557236195\n",
      "step: 690, loss: 0.005902268923819065\n",
      "step: 691, loss: 0.07521365582942963\n",
      "step: 692, loss: 0.077147476375103\n",
      "step: 693, loss: 0.010119778104126453\n",
      "step: 694, loss: 0.030214231461286545\n",
      "step: 695, loss: 0.005970746278762817\n",
      "step: 696, loss: 0.003818135941401124\n",
      "step: 697, loss: 0.027078520506620407\n",
      "step: 698, loss: 0.01306068804115057\n",
      "step: 699, loss: 0.019085021689534187\n",
      "step: 700, loss: 0.036837268620729446\n",
      "step: 701, loss: 0.00571949128061533\n",
      "step: 702, loss: 0.07184102386236191\n",
      "step: 703, loss: 0.014792433939874172\n",
      "step: 704, loss: 0.016174690797924995\n",
      "step: 705, loss: 0.11327795684337616\n",
      "step: 706, loss: 0.08708439767360687\n",
      "step: 707, loss: 0.005583179648965597\n",
      "step: 708, loss: 0.015339702367782593\n",
      "step: 709, loss: 0.006981242913752794\n",
      "step: 710, loss: 0.004135359078645706\n",
      "step: 711, loss: 0.023739762604236603\n",
      "step: 712, loss: 0.025623485445976257\n",
      "step: 713, loss: 0.11178078502416611\n",
      "step: 714, loss: 0.004380937200039625\n",
      "step: 715, loss: 0.13734182715415955\n",
      "step: 716, loss: 0.05644828453660011\n",
      "step: 717, loss: 0.05509217083454132\n",
      "step: 718, loss: 0.0147317536175251\n",
      "step: 719, loss: 0.03689069673418999\n",
      "step: 720, loss: 0.0028815106488764286\n",
      "step: 721, loss: 0.00537660950794816\n",
      "step: 722, loss: 0.002406930550932884\n",
      "step: 723, loss: 0.010439411737024784\n",
      "step: 724, loss: 0.010741357691586018\n",
      "step: 725, loss: 0.012819495052099228\n",
      "step: 726, loss: 0.01342915091663599\n",
      "step: 727, loss: 0.018448742106556892\n",
      "step: 728, loss: 0.1007005050778389\n",
      "step: 729, loss: 0.004282914102077484\n",
      "step: 730, loss: 0.04781920462846756\n",
      "step: 731, loss: 0.018881428986787796\n",
      "step: 732, loss: 0.021313771605491638\n",
      "step: 733, loss: 0.10469108819961548\n",
      "step: 734, loss: 0.00793230626732111\n",
      "step: 735, loss: 0.010952933691442013\n",
      "step: 736, loss: 0.010041993111371994\n",
      "step: 737, loss: 0.007298022974282503\n",
      "step: 738, loss: 0.019313151016831398\n",
      "step: 739, loss: 0.04214966297149658\n",
      "step: 740, loss: 0.033996157348155975\n",
      "step: 741, loss: 0.061445802450180054\n",
      "step: 742, loss: 0.03343275561928749\n",
      "step: 743, loss: 0.039852313697338104\n",
      "step: 744, loss: 0.019516881555318832\n",
      "step: 745, loss: 0.06289564818143845\n",
      "step: 746, loss: 0.0038599518593400717\n",
      "step: 747, loss: 0.004766466096043587\n",
      "step: 748, loss: 0.0020402451045811176\n",
      "step: 749, loss: 0.00347355124540627\n",
      "step: 750, loss: 0.007028819993138313\n",
      "step: 751, loss: 0.13345710933208466\n",
      "step: 752, loss: 0.057078905403614044\n",
      "step: 753, loss: 0.011977748945355415\n",
      "step: 754, loss: 0.1108647882938385\n",
      "step: 755, loss: 0.002237993525341153\n",
      "step: 756, loss: 0.003236117772758007\n",
      "step: 757, loss: 0.14767290651798248\n",
      "step: 758, loss: 0.09131616353988647\n",
      "step: 759, loss: 0.03426530212163925\n",
      "step: 760, loss: 0.004286795388907194\n",
      "step: 761, loss: 0.016855387017130852\n",
      "step: 762, loss: 0.018342938274145126\n",
      "step: 763, loss: 0.010592620819807053\n",
      "step: 764, loss: 0.013996546156704426\n",
      "step: 765, loss: 0.004433728288859129\n",
      "step: 766, loss: 0.005516007076948881\n",
      "step: 767, loss: 0.009608780033886433\n",
      "step: 768, loss: 0.032205693423748016\n",
      "step: 769, loss: 0.0036855130456387997\n",
      "step: 770, loss: 0.01722199097275734\n",
      "step: 771, loss: 0.04761376231908798\n",
      "step: 772, loss: 0.03354133665561676\n",
      "step: 773, loss: 0.01970377005636692\n",
      "step: 774, loss: 0.00881065335124731\n",
      "step: 775, loss: 0.005826245993375778\n",
      "step: 776, loss: 0.023840095847845078\n",
      "step: 777, loss: 0.029855284839868546\n",
      "step: 778, loss: 0.14297227561473846\n",
      "step: 779, loss: 0.0029170014895498753\n",
      "step: 780, loss: 0.031237099319696426\n",
      "step: 781, loss: 0.05238800123333931\n",
      "step: 782, loss: 0.03445784002542496\n",
      "step: 783, loss: 0.026550060138106346\n",
      "step: 784, loss: 0.011403106153011322\n",
      "step: 785, loss: 0.08930785953998566\n",
      "step: 786, loss: 0.017271902412176132\n",
      "step: 787, loss: 0.00822516530752182\n",
      "step: 788, loss: 0.0052824728190898895\n",
      "step: 789, loss: 0.012398915365338326\n",
      "step: 790, loss: 0.02811046876013279\n",
      "step: 791, loss: 0.004173461347818375\n",
      "step: 792, loss: 0.007019796874374151\n",
      "step: 793, loss: 0.20577029883861542\n",
      "step: 794, loss: 0.025113098323345184\n",
      "step: 795, loss: 0.03489460051059723\n",
      "step: 796, loss: 0.021143104881048203\n",
      "step: 797, loss: 0.027573060244321823\n",
      "step: 798, loss: 0.06368229538202286\n",
      "step: 799, loss: 0.06363845616579056\n",
      "step: 800, loss: 0.03351539745926857\n",
      "step: 801, loss: 0.0718899667263031\n",
      "step: 802, loss: 0.0030877410899847746\n",
      "step: 803, loss: 0.03575178235769272\n",
      "step: 804, loss: 0.11560476571321487\n",
      "step: 805, loss: 0.009211091324687004\n",
      "step: 806, loss: 0.05770432576537132\n",
      "step: 807, loss: 0.013188528828322887\n",
      "step: 808, loss: 0.012522914446890354\n",
      "step: 809, loss: 0.13483715057373047\n",
      "step: 810, loss: 0.0027867015451192856\n",
      "step: 811, loss: 0.004814540967345238\n",
      "step: 812, loss: 0.012057352811098099\n",
      "step: 813, loss: 0.024660784751176834\n",
      "step: 814, loss: 0.007956914603710175\n",
      "step: 815, loss: 0.006637296639382839\n",
      "step: 816, loss: 0.005399088840931654\n",
      "step: 817, loss: 0.06272763013839722\n",
      "step: 818, loss: 0.01791173778474331\n",
      "step: 819, loss: 0.03042900376021862\n",
      "step: 820, loss: 0.010678660124540329\n",
      "step: 821, loss: 0.05140043422579765\n",
      "step: 822, loss: 0.022039301693439484\n",
      "step: 823, loss: 0.05003460496664047\n",
      "step: 824, loss: 0.0019141838420182467\n",
      "step: 825, loss: 0.06291482597589493\n",
      "step: 826, loss: 0.003862692741677165\n",
      "step: 827, loss: 0.008350873365998268\n",
      "step: 828, loss: 0.0023762653581798077\n",
      "step: 829, loss: 0.011002843268215656\n",
      "step: 830, loss: 0.015870554372668266\n",
      "step: 831, loss: 0.1233873963356018\n",
      "step: 832, loss: 0.05011400207877159\n",
      "step: 833, loss: 0.01680668629705906\n",
      "step: 834, loss: 0.0166003555059433\n",
      "step: 835, loss: 0.05027751624584198\n",
      "step: 836, loss: 0.006813459564000368\n",
      "step: 837, loss: 0.06792128831148148\n",
      "step: 838, loss: 0.005026806145906448\n",
      "step: 839, loss: 0.03792579099535942\n",
      "step: 840, loss: 0.1924162656068802\n",
      "step: 841, loss: 0.02228524163365364\n",
      "step: 842, loss: 0.10579486191272736\n",
      "step: 843, loss: 0.01904529146850109\n",
      "step: 844, loss: 0.004491325002163649\n",
      "step: 845, loss: 0.005605640821158886\n",
      "step: 846, loss: 0.07644487172365189\n",
      "step: 847, loss: 0.016567490994930267\n",
      "step: 848, loss: 0.0752500519156456\n",
      "step: 849, loss: 0.008290394209325314\n",
      "step: 850, loss: 0.04198804497718811\n",
      "step: 851, loss: 0.004818021319806576\n",
      "step: 852, loss: 0.005328178405761719\n",
      "step: 853, loss: 0.07428041100502014\n",
      "step: 854, loss: 0.004387937020510435\n",
      "step: 855, loss: 0.0024362923577427864\n",
      "step: 856, loss: 0.006926930043846369\n",
      "step: 857, loss: 0.0037584772799164057\n",
      "step: 858, loss: 0.0024201618507504463\n",
      "step: 859, loss: 0.024810561910271645\n",
      "step: 860, loss: 0.04212622344493866\n",
      "step: 861, loss: 0.05462311580777168\n",
      "step: 862, loss: 0.017217593267560005\n",
      "step: 863, loss: 0.01654372178018093\n",
      "step: 864, loss: 0.0839628204703331\n",
      "step: 865, loss: 0.018045594915747643\n",
      "step: 866, loss: 0.004725031089037657\n",
      "step: 867, loss: 0.03672907501459122\n",
      "step: 868, loss: 0.03409197926521301\n",
      "step: 869, loss: 0.0022618682123720646\n",
      "step: 870, loss: 0.022284770384430885\n",
      "step: 871, loss: 0.02221498265862465\n",
      "step: 872, loss: 0.0033517631236463785\n",
      "step: 873, loss: 0.008860443718731403\n",
      "step: 874, loss: 0.001994274090975523\n",
      "step: 875, loss: 0.014083418995141983\n",
      "step: 876, loss: 0.22390002012252808\n",
      "step: 877, loss: 0.0031940052285790443\n",
      "step: 878, loss: 0.2019517868757248\n",
      "step: 879, loss: 0.01806408353149891\n",
      "step: 880, loss: 0.0050286496989429\n",
      "step: 881, loss: 0.006294512189924717\n",
      "step: 882, loss: 0.003307736711576581\n",
      "step: 883, loss: 0.018271420150995255\n",
      "step: 884, loss: 0.06996630132198334\n",
      "step: 885, loss: 0.004277844913303852\n",
      "step: 886, loss: 0.006375845056027174\n",
      "step: 887, loss: 0.08731023967266083\n",
      "step: 888, loss: 0.0513167567551136\n",
      "step: 889, loss: 0.03326215595006943\n",
      "step: 890, loss: 0.002519679255783558\n",
      "step: 891, loss: 0.0008902656263671815\n",
      "step: 892, loss: 0.1353498101234436\n",
      "step: 893, loss: 0.04126723110675812\n",
      "step: 894, loss: 0.006473871413618326\n",
      "step: 895, loss: 0.015673890709877014\n",
      "step: 896, loss: 0.010357456281781197\n",
      "step: 897, loss: 0.07383009046316147\n",
      "step: 898, loss: 0.009731010533869267\n",
      "step: 899, loss: 0.005435734987258911\n",
      "step: 900, loss: 0.06459527462720871\n",
      "step: 901, loss: 0.0023864840622991323\n",
      "step: 902, loss: 0.09507424384355545\n",
      "step: 903, loss: 0.001480050035752356\n",
      "step: 904, loss: 0.008266028016805649\n",
      "step: 905, loss: 0.004797574132680893\n",
      "step: 906, loss: 0.0015330561436712742\n",
      "step: 907, loss: 0.0031072639394551516\n",
      "step: 908, loss: 0.0037382247392088175\n",
      "step: 909, loss: 0.001840417506173253\n",
      "step: 910, loss: 0.0011159501736983657\n",
      "step: 911, loss: 0.0014550506602972746\n",
      "step: 912, loss: 0.14537465572357178\n",
      "step: 913, loss: 0.007661257870495319\n",
      "step: 914, loss: 0.0032157900277525187\n",
      "step: 915, loss: 0.03720421344041824\n",
      "step: 916, loss: 0.0305420383810997\n",
      "step: 917, loss: 0.0035040476359426975\n",
      "step: 918, loss: 0.004318735096603632\n",
      "step: 919, loss: 0.01348456833511591\n",
      "step: 920, loss: 0.0022280740085989237\n",
      "step: 921, loss: 0.017374826595187187\n",
      "step: 922, loss: 0.030422374606132507\n",
      "step: 923, loss: 0.006492906250059605\n",
      "step: 924, loss: 0.015045174397528172\n",
      "step: 925, loss: 0.014150649309158325\n",
      "step: 926, loss: 0.0021474254317581654\n",
      "step: 927, loss: 0.027352314442396164\n",
      "step: 928, loss: 0.004016106948256493\n",
      "step: 929, loss: 0.004886924754828215\n",
      "step: 930, loss: 0.002012125216424465\n",
      "step: 931, loss: 0.014783396385610104\n",
      "step: 932, loss: 0.042928121984004974\n",
      "step: 933, loss: 0.0015811362536624074\n",
      "step: 934, loss: 0.0034758010879158974\n",
      "step: 935, loss: 0.0010803556069731712\n",
      "step: 936, loss: 0.0024292380549013615\n",
      "step: 937, loss: 0.0023014931939542294\n",
      "step: 938, loss: 0.002381913596764207\n",
      "step: 939, loss: 0.0011080667609348893\n",
      "step: 940, loss: 0.0037435200065374374\n",
      "step: 941, loss: 0.0019385177874937654\n",
      "step: 942, loss: 0.0023431689478456974\n",
      "step: 943, loss: 0.002335335360839963\n",
      "step: 944, loss: 0.05656323581933975\n",
      "step: 945, loss: 0.0015857205726206303\n",
      "step: 946, loss: 0.001234312541782856\n",
      "step: 947, loss: 0.0030428054742515087\n",
      "step: 948, loss: 0.0012446671025827527\n",
      "step: 949, loss: 0.001839259872213006\n",
      "step: 950, loss: 0.0024461376015096903\n",
      "step: 951, loss: 0.003092156955972314\n",
      "step: 952, loss: 0.06679517030715942\n",
      "step: 953, loss: 0.0018977755680680275\n",
      "step: 954, loss: 0.002938017016276717\n",
      "step: 955, loss: 0.05120924487709999\n",
      "step: 956, loss: 0.002308364026248455\n",
      "step: 957, loss: 0.0021989839151501656\n",
      "step: 958, loss: 0.00352425966411829\n",
      "step: 959, loss: 0.0011278041638433933\n",
      "step: 960, loss: 0.006864352151751518\n",
      "step: 961, loss: 0.0030273732263594866\n",
      "step: 962, loss: 0.004892689175903797\n",
      "step: 963, loss: 0.003279856638982892\n",
      "step: 964, loss: 0.012900820933282375\n",
      "step: 965, loss: 0.0017734094290062785\n",
      "step: 966, loss: 0.056043319404125214\n",
      "step: 967, loss: 0.0015744847478345037\n",
      "step: 968, loss: 0.003943042363971472\n",
      "step: 969, loss: 0.038398243486881256\n",
      "step: 970, loss: 0.0014068265445530415\n",
      "step: 971, loss: 0.0006987449596635997\n",
      "step: 972, loss: 0.06044214218854904\n",
      "step: 973, loss: 0.04862835258245468\n",
      "step: 974, loss: 0.004692263435572386\n",
      "step: 975, loss: 0.0036867461167275906\n",
      "step: 976, loss: 0.0026065059937536716\n",
      "step: 977, loss: 0.004553557839244604\n",
      "step: 978, loss: 0.03724564611911774\n",
      "step: 979, loss: 0.0010813380358740687\n",
      "step: 980, loss: 0.006832776125520468\n",
      "step: 981, loss: 0.04140034317970276\n",
      "step: 982, loss: 0.0014562230790033937\n",
      "step: 983, loss: 0.09326986223459244\n",
      "step: 984, loss: 0.0006359528633765876\n",
      "step: 985, loss: 0.0028047007508575916\n",
      "step: 986, loss: 0.04392969608306885\n",
      "step: 987, loss: 0.0028181683737784624\n",
      "step: 988, loss: 0.004586475435644388\n",
      "step: 989, loss: 0.0023201063740998507\n",
      "step: 990, loss: 0.00482852291315794\n",
      "step: 991, loss: 0.0016760129947215319\n",
      "step: 992, loss: 0.011698267422616482\n",
      "step: 993, loss: 0.0010582809336483479\n",
      "step: 994, loss: 0.002939827973023057\n",
      "step: 995, loss: 0.005988400429487228\n",
      "step: 996, loss: 0.06069320812821388\n",
      "step: 997, loss: 0.0010405783541500568\n",
      "step: 998, loss: 0.009602890349924564\n",
      "step: 999, loss: 0.008870231918990612\n",
      "step: 1000, loss: 0.0028168188873678446\n",
      "EpochL 4\n",
      "Accuracy on test data: 0.7883696229421137\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    NUM_CLASSES = 20\n",
    "    \n",
    "    with open('../Session1/data/20news-bydate/words_idfs.txt') as f:\n",
    "        vocab_size = len(f.read().splitlines())\n",
    "        \n",
    "    mlp = MLP(vocab_size, 50)\n",
    "    \n",
    "    predicted_labels, loss = mlp.build_graph()\n",
    "    train_op = mlp.trainer(loss, 0.1)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        train_data_reader, test_data_reader = load_dataset()\n",
    "        step, MAX_STEP = 0, 1000\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        while step < MAX_STEP:\n",
    "            train_data, train_labels = train_data_reader.next_batch()\n",
    "            plabels_eval, loss_eval, _ = sess.run(\n",
    "                [predicted_labels, loss, train_op],\n",
    "                feed_dict={\n",
    "                    mlp._X: train_data,\n",
    "                    mlp._real_Y: train_labels\n",
    "                }\n",
    "            )\n",
    "            step += 1\n",
    "            print(f'step: {step}, loss: {loss_eval}')\n",
    "            \n",
    "            trainable_variables = tf.trainable_variables()\n",
    "            for variable in trainable_variables:\n",
    "                save_parameters(\n",
    "                    name=variable.name,\n",
    "                    value=variable.eval(),\n",
    "                    epoch=train_data_reader._num_epoch\n",
    "                )\n",
    "                \n",
    "    test_data_reader = DataReader(\n",
    "        '../Session1/data/20news-bydate/tfidf_test.txt',\n",
    "        50, vocab_size\n",
    "    )\n",
    "                \n",
    "    with tf.Session() as sess:        \n",
    "        epoch = train_data_reader._num_epoch\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        for variable in trainable_variables:\n",
    "            saved_value = restore_parameters(variable.name, epoch)\n",
    "            assign_op = variable.assign(saved_value)\n",
    "            sess.run(assign_op)\n",
    "        \n",
    "        num_true_preds = 0\n",
    "        while True:\n",
    "            test_data, test_labels = test_data_reader.next_batch()\n",
    "            test_plabels_eval = sess.run(\n",
    "                predicted_labels,\n",
    "                feed_dict={\n",
    "                    mlp._X: test_data,\n",
    "                    mlp._real_Y: test_labels\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            matches = np.equal(test_plabels_eval, test_labels)\n",
    "            num_true_preds += np.sum(matches.astype(float))\n",
    "            \n",
    "            if test_data_reader._batch_id == 0:\n",
    "                break\n",
    "                \n",
    "        print(f'EpochL {epoch}')\n",
    "        print(f'Accuracy on test data: {num_true_preds / len(test_data_reader._data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72299b1-0f97-422f-b445-4e2247299d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
