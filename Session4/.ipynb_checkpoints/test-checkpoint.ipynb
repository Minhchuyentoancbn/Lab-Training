{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "011c5973-3726-4859-86b5-8d8835882f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "192a7122-6e71-4906-9ebf-5c56fb8b5719",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DOC_LENGTH = 500\n",
    "NUM_CLASSES = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1f3ec4-7760-4613-9526-373d7b4c592e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "158935e6-a170-4425-9668-091b0d978c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 0-alt.atheism\n",
      "Processing: 1-comp.graphics\n",
      "Processing: 2-comp.os.ms-windows.misc\n",
      "Processing: 3-comp.sys.ibm.pc.hardware\n",
      "Processing: 4-comp.sys.mac.hardware\n",
      "Processing: 5-comp.windows.x\n",
      "Processing: 6-misc.forsale\n",
      "Processing: 7-rec.autos\n",
      "Processing: 8-rec.motorcycles\n",
      "Processing: 9-rec.sport.baseball\n",
      "Processing: 10-rec.sport.hockey\n",
      "Processing: 11-sci.crypt\n",
      "Processing: 12-sci.electronics\n",
      "Processing: 13-sci.med\n",
      "Processing: 14-sci.space\n",
      "Processing: 15-soc.religion.christian\n",
      "Processing: 16-talk.politics.guns\n",
      "Processing: 17-talk.politics.mideast\n",
      "Processing: 18-talk.politics.misc\n",
      "Processing: 19-talk.religion.misc\n",
      "Processing: 0-alt.atheism\n",
      "Processing: 1-comp.graphics\n",
      "Processing: 2-comp.os.ms-windows.misc\n",
      "Processing: 3-comp.sys.ibm.pc.hardware\n",
      "Processing: 4-comp.sys.mac.hardware\n",
      "Processing: 5-comp.windows.x\n",
      "Processing: 6-misc.forsale\n",
      "Processing: 7-rec.autos\n",
      "Processing: 8-rec.motorcycles\n",
      "Processing: 9-rec.sport.baseball\n",
      "Processing: 10-rec.sport.hockey\n",
      "Processing: 11-sci.crypt\n",
      "Processing: 12-sci.electronics\n",
      "Processing: 13-sci.med\n",
      "Processing: 14-sci.space\n",
      "Processing: 15-soc.religion.christian\n",
      "Processing: 16-talk.politics.guns\n",
      "Processing: 17-talk.politics.mideast\n",
      "Processing: 18-talk.politics.misc\n",
      "Processing: 19-talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "gen_data_and_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad9deb39-1053-414f-b671-11e3a6c7a418",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_data('data/w2v/20news-train-raw.txt', 'data/w2v/vocab-raw.txt')\n",
    "encode_data('data/w2v/20news-test-raw.txt', 'data/w2v/vocab-raw.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09ff659-c577-48dd-ae4c-aecfb12c93b7",
   "metadata": {},
   "source": [
    "# 2. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6590f792-7d93-4ab5-b651-4babab7bf7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\anacoda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4ef8dcd-6ded-4d16-a065-e9678e1c12ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, vocab_size, embedding_size, \n",
    "                 lstm_size, batch_size):\n",
    "        self._vocab_size = vocab_size\n",
    "        self._embedding_size = embedding_size\n",
    "        self._lstm_size = lstm_size\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        self._data = tf.placeholder(tf.int32, shape=[batch_size, MAX_DOC_LENGTH])\n",
    "        self._labels = tf.placeholder(tf.int32, shape=[batch_size, ])\n",
    "        self._sentence_lengths = tf.placeholder(tf.int32, shape=[batch_size, ])\n",
    "        self._final_tokens = tf.placeholder(tf.int32, shape=[batch_size, ])\n",
    "\n",
    "\n",
    "    def embedding_layer(self, indices):\n",
    "        pretrained_vectors = []\n",
    "        pretrained_vectors.append(np.zeros(self._embedding_size))\n",
    "        np.random.seed(2022)\n",
    "        for _ in range(self._vocab_size + 1):\n",
    "            pretrained_vectors.append(np.random.normal(size=self._embedding_size))\n",
    "        \n",
    "        pretrained_vectors = np.array(pretrained_vectors)\n",
    "        \n",
    "        self._embedding_matrix = tf.get_variable(\n",
    "            name='embedding',\n",
    "            shape=(self._vocab_size + 2, self._embedding_size),\n",
    "            initializer=tf.constant_initializer(pretrained_vectors)\n",
    "        )\n",
    "        \n",
    "        return tf.nn.embedding_lookup(self._embedding_matrix, indices)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def LSTM_layer(self, embeddings):\n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(self._lstm_size)\n",
    "        zero_state = tf.zeros(shape=(self._batch_size, self._lstm_size))\n",
    "        initial_state = tf.nn.rnn_cell.LSTMStateTuple(zero_state, zero_state)\n",
    "        \n",
    "        lstm_inputs = tf.unstack(\n",
    "            tf.transpose(embeddings, perm=[1, 0, 2])\n",
    "        )\n",
    "        lstm_outputs, last_state = tf.nn.static_rnn(\n",
    "            cell=lstm_cell,\n",
    "            inputs=lstm_inputs,\n",
    "            initial_state=initial_state,\n",
    "            sequence_length=self._sentence_lengths\n",
    "        )\n",
    "        \n",
    "        lstm_outputs = tf.unstack(\n",
    "            tf.transpose(lstm_outputs, perm=[1, 0, 2])\n",
    "        )\n",
    "        lstm_outputs = tf.concat(lstm_outputs, axis=0)\n",
    "        \n",
    "        mask = tf.sequence_mask(\n",
    "            lengths=self._sentence_lengths,\n",
    "            maxlen=MAX_DOC_LENGTH,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        mask = tf.concat(tf.unstack(mask, axis=0), axis=0)\n",
    "        mask = tf.expand_dims(mask, -1)\n",
    "        \n",
    "        lstm_outputs = mask * lstm_outputs\n",
    "        lstm_outputs_split = tf.split(lstm_outputs, num_or_size_splits=self._batch_size)\n",
    "        lstm_outputs_sum = tf.reduce_sum(lstm_outputs_split, axis=1)\n",
    "        lstm_outputs_average = lstm_outputs_sum / tf.expand_dims(\n",
    "            tf.cast(self._sentence_lengths, tf.float32),\n",
    "            -1\n",
    "        )\n",
    "        return lstm_outputs_average\n",
    "\n",
    "    \n",
    "    \n",
    "    def build_graph(self):\n",
    "        embeddings = self.embedding_layer(self._data)\n",
    "        lstm_outputs = self.LSTM_layer(embeddings)\n",
    "        \n",
    "        weights = tf.get_variable(\n",
    "            name='final_layer_weights',\n",
    "            shape=(self._lstm_size, NUM_CLASSES),\n",
    "            initializer=tf.random_normal_initializer(seed=2022)\n",
    "        )\n",
    "        biases = tf.get_variable(\n",
    "            name='final_layer_biases',\n",
    "            shape=(NUM_CLASSES),\n",
    "            initializer=tf.random_normal_initializer(seed=2022)\n",
    "        )\n",
    "        \n",
    "        logits = tf.matmul(lstm_outputs, weights) + biases\n",
    "        \n",
    "        labels_one_hot = tf.one_hot(\n",
    "            indices=self._labels,\n",
    "            depth=NUM_CLASSES,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=labels_one_hot,\n",
    "            logits=logits\n",
    "        )\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        probs = tf.nn.softmax(logits)\n",
    "        predicted_labels = tf.argmax(probs, axis=1)\n",
    "        predicted_labels = tf.squeeze(predicted_labels)\n",
    "        \n",
    "        return predicted_labels, loss\n",
    "\n",
    "    \n",
    "    def trainer(self, loss, learning_rate):\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        return train_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32238cb7-a681-47c4-88d0-3d9c25609a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "    def __init__(self, data_path, batch_size):\n",
    "        self._batch_size = batch_size\n",
    "        with open(data_path) as f:\n",
    "            d_lines = f.read().splitlines()\n",
    "            \n",
    "        self._data = []\n",
    "        self._labels = []\n",
    "        self._sentence_lengths = []\n",
    "        self._final_tokens = []\n",
    "        for data_id, line in enumerate(d_lines):\n",
    "            features = line.split('<fff>')\n",
    "            label, doc_id, sentence_length = int(features[0]), int(features[1]), int(features[2])\n",
    "            tokens = features[3].split()\n",
    "\n",
    "            self._data.append(tokens)\n",
    "            self._sentence_lengths.append(sentence_length)\n",
    "            self._labels.append(label)\n",
    "            self._final_tokens.append(tokens[-1])\n",
    "        \n",
    "        self._data = np.array(self._data)\n",
    "        self._labels = np.array(self._labels)\n",
    "\n",
    "        self._sentence_lengths = np.array(self._sentence_lengths)\n",
    "        self._final_tokens = np.array(self._final_tokens)\n",
    "        \n",
    "        self._num_epoch = 0\n",
    "        self._batch_id = 0\n",
    "        self._size = len(self._data)\n",
    "    \n",
    "    def next_batch(self):\n",
    "        start = self._batch_id * self._batch_size\n",
    "        end = start + self._batch_size\n",
    "        self._batch_id += 1\n",
    "        \n",
    "        if end + self._batch_size > len(self._data):\n",
    "            self._size = end\n",
    "            end = len(self._data)\n",
    "            start = end - self._batch_size\n",
    "            self._num_epoch += 1\n",
    "            self._batch_id = 0\n",
    "            indices = list(range(len(self._data)))\n",
    "            random.seed(2022)\n",
    "            random.shuffle(indices)\n",
    "            self._data, self._labels, self._sentence_lengths, self._final_tokens = \\\n",
    "                self._data[indices], self._labels[indices], self._sentence_lengths[indices], self._final_tokens[indices]\n",
    "            \n",
    "        \n",
    "        return self._data[start:end], self._labels[start:end], self._sentence_lengths[start:end], self._final_tokens[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23f94b8c-e722-448c-b848-90eed84e6fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_RNN():\n",
    "    with open('data/w2v/vocab-raw.txt') as f:\n",
    "        vocab_size = len(f.read().splitlines())\n",
    "        \n",
    "    tf.set_random_seed(2022)\n",
    "    rnn = RNN(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_size=300,\n",
    "        lstm_size=50,\n",
    "        batch_size=50\n",
    "    )\n",
    "    predicted_labels, loss = rnn.build_graph()\n",
    "    train_op = rnn.trainer(loss=loss, learning_rate=0.01)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        train_data_reader = DataReader(\n",
    "            data_path='data/w2v/20news-train-encoded.txt',\n",
    "            batch_size=50\n",
    "        )\n",
    "        \n",
    "        test_data_reader = DataReader(\n",
    "            data_path='data/w2v/20news-test-encoded.txt',\n",
    "            batch_size=50\n",
    "        )\n",
    "        step = 0\n",
    "        MAX_STEP = 10000\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        while step < MAX_STEP:\n",
    "            next_train_batch = train_data_reader.next_batch()\n",
    "            train_data, train_labels, train_sentence_lengths, train_final_tokens = next_train_batch\n",
    "            plabels_eval, loss_eval, _ = sess.run(\n",
    "                [predicted_labels, loss, train_op],\n",
    "                feed_dict={\n",
    "                    rnn._data: train_data,\n",
    "                    rnn._labels: train_labels,\n",
    "                    rnn._sentence_lengths: train_sentence_lengths,\n",
    "                    rnn._final_tokens: train_final_tokens\n",
    "                }\n",
    "            )\n",
    "            step += 1\n",
    "            if step % 20 == 0:\n",
    "                print(f'loss : {loss_eval}')\n",
    "            if train_data_reader._batch_id == 0:\n",
    "                num_true_preds = 0\n",
    "                while True:\n",
    "                    next_test_batch = test_data_reader.next_batch()\n",
    "                    test_data, test_labels, test_sentence_lengths, test_final_tokens = next_test_batch\n",
    "                    \n",
    "                    test_plabels_eval = sess.run(\n",
    "                        predicted_labels,\n",
    "                        feed_dict={\n",
    "                            rnn._data: test_data,\n",
    "                            rnn._labels: test_labels,\n",
    "                            rnn._sentence_lengths: test_sentence_lengths,\n",
    "                            rnn._final_tokens: test_final_tokens\n",
    "                        }\n",
    "                    )\n",
    "                    matches = np.equal(test_plabels_eval, test_labels)\n",
    "                    num_true_preds += np.sum(matches.astype(float))\n",
    "                    \n",
    "                    if test_data_reader._batch_id == 0:\n",
    "                        break\n",
    "                    \n",
    "                print(f'Epoch: {train_data_reader._num_epoch}')\n",
    "                print(f'Accuracy on test data: {num_true_preds * 100. / len(test_data_reader._data)}')\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccda620-30a7-46df-8832-8e0997e652be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18640\\809615805.py:42: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From F:\\anacoda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:750: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anacoda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:699: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be \"\n",
      "F:\\anacoda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\base_layer_v1.py:1684: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\anacoda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "loss : 0.0001464664237573743\n",
      "loss : 0.04266733676195145\n",
      "loss : 10.1600980758667\n",
      "loss : 0.19843825697898865\n",
      "loss : 3.1871166229248047\n",
      "loss : 4.675755500793457\n",
      "loss : 2.536137104034424\n",
      "loss : 2.6067628860473633\n",
      "loss : 4.922901153564453\n",
      "loss : 4.948700428009033\n",
      "loss : 5.2951555252075195\n",
      "Epoch: 1\n",
      "Accuracy on test data: 6.200212426978227\n",
      "loss : 2.717081308364868\n",
      "loss : 2.506956100463867\n",
      "loss : 2.2392189502716064\n",
      "loss : 2.1006577014923096\n",
      "loss : 1.8725680112838745\n",
      "loss : 1.9692553281784058\n",
      "loss : 1.731837272644043\n",
      "loss : 1.748887062072754\n",
      "loss : 1.721579909324646\n",
      "loss : 1.7344409227371216\n",
      "loss : 1.2171874046325684\n",
      "Epoch: 2\n",
      "Accuracy on test data: 67.1003717472119\n",
      "loss : 1.0214741230010986\n",
      "loss : 1.0996121168136597\n",
      "loss : 1.0642610788345337\n",
      "loss : 0.8575885891914368\n",
      "loss : 1.0136315822601318\n",
      "loss : 0.62203049659729\n",
      "loss : 0.6855430603027344\n",
      "loss : 0.8849252462387085\n",
      "loss : 0.8177310824394226\n",
      "loss : 0.6166874766349792\n",
      "loss : 0.6075671315193176\n",
      "Epoch: 3\n",
      "Accuracy on test data: 73.52628783855549\n",
      "loss : 0.45561572909355164\n",
      "loss : 0.3518528640270233\n",
      "loss : 0.35233646631240845\n",
      "loss : 0.40621864795684814\n",
      "loss : 0.3359852731227875\n",
      "loss : 0.37650245428085327\n",
      "loss : 0.3519288897514343\n",
      "loss : 0.35578683018684387\n",
      "loss : 0.2984532415866852\n",
      "loss : 0.23245544731616974\n",
      "loss : 0.308471143245697\n",
      "loss : 0.24544329941272736\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_RNN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
